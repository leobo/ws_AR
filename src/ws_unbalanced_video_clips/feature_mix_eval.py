import os

import numpy as np
import tensorflow as tf
from sklearn import preprocessing

from trainTestSamplesGen import TrainTestSampleGen
from weighted_sum.videoDescriptorWeightedSum import Weightedsum
from ws_unbalanced_video_clips.feature_mix_classifier import classify

dims = 3
num_train_data = 9537
num_test_data = 3783
num_samples_per_training_video = 1
num_samples_per_testing_video = 1


def _int64_feature(value):
    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))


def _bytes_feature(value):
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))


def main(rgb_path_list, u_path_list, v_path_list, train_sp, test_sp, train_test_splits, dim, tMatrices_list,
         dataset='ucf'):
    # generate train test splits for different datasets.
    if dataset == 'hmdb':
        tts = TrainTestSampleGen(ucf_path='', hmdb_path=train_test_splits)
    else:
        tts = TrainTestSampleGen(ucf_path=train_test_splits, hmdb_path='')

    # there are 3 train test splits for each of ucf101 and hmdb51
    encoder = preprocessing.LabelEncoder()
    for split_num in range(1):
        # for test purpose
        # train_videos = tts.train_data_label[split_num]['data'][:10]
        # train_labels = encoder.fit_transform(tts.train_data_label[split_num]['label'][:10])
        # test_videos = tts.test_data_label[split_num]['data'][:10]
        # test_labels = encoder.fit_transform(tts.test_data_label[split_num]['label'][:10])

        train_videos = tts.train_data_label[split_num]['data']
        train_labels = encoder.fit_transform(tts.train_data_label[split_num]['label'])
        test_videos = tts.test_data_label[split_num]['data']
        test_labels = encoder.fit_transform(tts.test_data_label[split_num]['label'])

        # generate and write video descriptor for training videos
        # create tf_record writer
        writer = tf.python_io.TFRecordWriter(os.path.join(train_sp, "rgb_flow_labels.tfrecord"))
        for video, label in zip(train_videos, train_labels):
            # rgb, flow u and flow v features containers.
            features = []
            # each video have multiple frame features which are generated by random crop the frames.
            for tm in tMatrices_list:
                seed = np.random.randint(1, 100)
                for rgb_p, u_p, v_p in zip(rgb_path_list, u_path_list, v_path_list):
                    rgb_ff = np.load(os.path.join(rgb_p, video + '.npy'))
                    u_ff = np.load(os.path.join(u_p, video + '.npy'))
                    v_ff = np.load(os.path.join(v_p, video + '.npy'))
                    tran_m_rgb = Weightedsum(None, None, None).attension_weights_gen(dims, len(rgb_ff), seed)
                    tran_m_flow = Weightedsum(None, None, None).attension_weights_gen(dims, len(u_ff), seed)
                    f = np.stack(
                        (
                            preprocessing.normalize(
                                Weightedsum(video, rgb_ff, None).ws_descriptor_gen(dim, False, tran_m_rgb), axis=0, norm='max'),
                            preprocessing.normalize(
                                Weightedsum(video, u_ff, None).ws_descriptor_gen(dim, False, tran_m_flow), axis=0, norm='max'),
                            preprocessing.normalize(
                                Weightedsum(video, v_ff, None).ws_descriptor_gen(dim, False, tran_m_flow), axis=0, norm='max')
                        ), axis=0
                    )
                    # dims*2048*3
                    features.append(np.swapaxes(f, 0, -1))

            features = np.reshape(features, [len(rgb_path_list) * len(tMatrices_list) * dim, 2048, 3])
            feature = {
                'features': _bytes_feature(features.astype(np.float32).tobytes()),
                'label': _int64_feature(label)
            }
            # write the video descriptors for current video
            # create an example protocol buffer
            example = tf.train.Example(features=tf.train.Features(feature=feature))
            # Serialize to string and write on the file
            writer.write(example.SerializeToString())
        writer.close()
        print("Training data generation done!")

        # generate and write video descriptor for test videos
        # create tf_record writer
        writer = tf.python_io.TFRecordWriter(os.path.join(test_sp, "rgb_flow_labels.tfrecord"))
        for video, label in zip(test_videos, test_labels):
            # rgb, flow u and flow v features containers.
            features = []
            # each video have multiple frame features which are generated by random crop the frames.
            for tm in tMatrices_list:
                seed = np.random.randint(1, 100)
                for rgb_p, u_p, v_p in zip(rgb_path_list, u_path_list, v_path_list):
                    rgb_ff = np.load(os.path.join(rgb_p, video + '.npy'))
                    u_ff = np.load(os.path.join(u_p, video + '.npy'))
                    v_ff = np.load(os.path.join(v_p, video + '.npy'))
                    tran_m_rgb = Weightedsum(None, None, None).attension_weights_gen(dims, len(rgb_ff), seed)
                    tran_m_flow = Weightedsum(None, None, None).attension_weights_gen(dims, len(u_ff), seed)
                    f = np.stack(
                        (
                            preprocessing.normalize(
                                Weightedsum(video, rgb_ff, None).ws_descriptor_gen(dim, False, tran_m_rgb), axis=0,
                                norm='max'),
                            preprocessing.normalize(
                                Weightedsum(video, u_ff, None).ws_descriptor_gen(dim, False, tran_m_flow), axis=0,
                                norm='max'),
                            preprocessing.normalize(
                                Weightedsum(video, v_ff, None).ws_descriptor_gen(dim, False, tran_m_flow), axis=0,
                                norm='max')
                        ), axis=0
                    )
                    # dims*2048*3
                    features.append(np.swapaxes(f, 0, -1))

            features = np.reshape(features, [len(rgb_path_list) * len(tMatrices_list) * dim, 2048, 3])
            feature = {
                'features': _bytes_feature(features.astype(np.float32).tobytes()),
                'label': _int64_feature(label)
            }
            # write the video descriptors for current video
            # create an example protocol buffer
            example = tf.train.Example(features=tf.train.Features(feature=feature))
            # Serialize to string and write on the file
            writer.write(example.SerializeToString())
        writer.close()
        print("Testing data generation done!")


if __name__ == '__main__':
    ucf_resNet_flow_crop_save_path_1_v4 = "/home/boy2/UCF101/ucf101_dataset/frame_features/resNet_flow_crop_v4/u"
    ucf_resNet_flow_crop_save_path_2_v4 = "/home/boy2/UCF101/ucf101_dataset/frame_features/resNet_flow_crop_v4/v"
    ucf_resNet_crop_save_path_v4 = "/home/boy2/UCF101/ucf101_dataset/frame_features/resNet_crop_v4"

    ucf_resNet_flow_crop_save_path_1_v3 = "/home/boy2/UCF101/ucf101_dataset/frame_features/resNet_flow_crop_v3/u"
    ucf_resNet_flow_crop_save_path_2_v3 = "/home/boy2/UCF101/ucf101_dataset/frame_features/resNet_flow_crop_v3/v"
    ucf_resNet_crop_save_path_v3 = "/home/boy2/UCF101/ucf101_dataset/frame_features/resNet_crop_v3"

    ucf_resNet_flow_crop_save_path_1_v2 = "/home/boy2/UCF101/ucf101_dataset/frame_features/resNet_flow_crop_v2/u"
    ucf_resNet_flow_crop_save_path_2_v2 = "/home/boy2/UCF101/ucf101_dataset/frame_features/resNet_flow_crop_v2/v"
    ucf_resNet_crop_save_path_v2 = "/home/boy2/UCF101/ucf101_dataset/frame_features/resNet_crop_v2"

    ucf_resNet_flow_crop_save_path_1_v1 = "/home/boy2/UCF101/ucf101_dataset/frame_features/resNet_flow_crop/u"
    ucf_resNet_flow_crop_save_path_2_v1 = "/home/boy2/UCF101/ucf101_dataset/frame_features/resNet_flow_crop/v"
    ucf_resNet_crop_save_path_v1 = "/home/boy2/UCF101/ucf101_dataset/frame_features/resNet_crop"

    ucf_resNet_flow_save_path_1 = "/home/boy2/UCF101/ucf101_dataset/frame_features/resNet_flow/u"
    ucf_resNet_flow_save_path_2 = "/home/boy2/UCF101/ucf101_dataset/frame_features/resNet_flow/v"
    ucf_resNet_save_path = "/home/boy2/UCF101/ucf101_dataset/frame_features/resNet"

    featureStorePath = ["/home/boy2/UCF101/ucf101_dataset/frame_features/resNet_resize",
                        "/home/boy2/UCF101/ucf101_dataset/frame_features/resNet_flow_resize/u",
                        "/home/boy2/UCF101/ucf101_dataset/frame_features/resNet_flow_resize/v"]
    featureStorePathFlip = ["/home/boy2/UCF101/ucf101_dataset/frame_features/resNet_resize_flip",
                            "/home/boy2/UCF101/ucf101_dataset/frame_features/resNet_flow_resize_flip/u",
                            "/home/boy2/UCF101/ucf101_dataset/frame_features/resNet_flow_resize_flip/v"]

    train_tfRecord_save_path = "/home/boy2/UCF101/ucf101_dataset/features/unbalanced_video_clips_train_v1"
    test_tfRecord_save_path = "/home/boy2/UCF101/ucf101_dataset/features/unbalanced_video_clips_test_v1"

    ucf_resNet_train_path_v2 = "/home/boy2/UCF101/ucf101_dataset/features/unbalanced_video_clips_train_v2"
    ucf_resNet_test_path_v2 = "/home/boy2/UCF101/ucf101_dataset/features/unbalanced_video_clips_test_v2"

    ucf_resNet_train_path_v3 = "/home/boy2/UCF101/ucf101_dataset/features/unbalanced_video_clips_train_v3"
    ucf_resNet_test_path_v3 = "/home/boy2/UCF101/ucf101_dataset/features/unbalanced_video_clips_test_v3"

    ucf_resNet_train_path_v4 = "/home/boy2/UCF101/ucf101_dataset/features/unbalanced_video_clips_train_v4"
    ucf_resNet_test_path_v4 = "/home/boy2/UCF101/ucf101_dataset/features/unbalanced_video_clips_test_v4"

    ucf_train_test_splits_save_path = "/home/boy2/UCF101/ucf101_dataset/features/testTrainSplits"

    selected_frames_path = '/home/boy2/UCF101/ucf101_dataset/frame_features/seq_number'

    # remove_dirctories([ucf_resNet_crop_ws_save_path_v1, ucf_resNet_crop_flow_ws_save_path_1_v1,
    #                    ucf_resNet_crop_flow_ws_save_path_2_v1,
    #                    ucf_resNet_crop_ws_save_path_v2, ucf_resNet_crop_flow_ws_save_path_1_v2,
    #                    ucf_resNet_crop_flow_ws_save_path_2_v2])

    rgb_feature_list = [[ucf_resNet_crop_save_path_v1, ucf_resNet_crop_save_path_v3, ucf_resNet_crop_save_path_v4][0]]
    u_feature_list = [[ucf_resNet_flow_crop_save_path_1_v1, ucf_resNet_flow_crop_save_path_1_v3,
                      ucf_resNet_flow_crop_save_path_1_v4][0]]
    v_feature_list = [[ucf_resNet_flow_crop_save_path_2_v1, ucf_resNet_flow_crop_save_path_2_v3,
                      ucf_resNet_flow_crop_save_path_2_v4][0]]

    best_acc = 0
    acc_list = []
    for i in range(1):
        # the max length without selection flow is 1776, with selection flow is 1150
        seed_1 = np.random.randint(1, 100)
        seed_2 = np.random.randint(1, 100)
        print("The seed 1 for trans_matrix is:", seed_1, "The seed 2 for trans_matrix is:", seed_2)
        tran_m_1 = Weightedsum('tran_m', [], None).transformation_matrix_gen(dims, 1776, seed_1)
        tran_m_1_norm = preprocessing.normalize(tran_m_1, axis=1)
        tran_m_2 = Weightedsum('tran_m', [], None).transformation_matrix_gen(dims, 1776, seed_2)
        tran_m_2_norm = preprocessing.normalize(tran_m_2, axis=1)

        tMatrices = [tran_m_1_norm, tran_m_2_norm]

        main(rgb_feature_list, u_feature_list, v_feature_list, train_tfRecord_save_path, test_tfRecord_save_path,
             ucf_train_test_splits_save_path, dims, tMatrices)
        accuracy = classify(
            os.path.join(train_tfRecord_save_path, "rgb_flow_labels.tfrecord"),
            os.path.join(test_tfRecord_save_path, "rgb_flow_labels.tfrecord"),
            num_train_data * num_samples_per_training_video,
            num_test_data * num_samples_per_testing_video,
            num_samples_per_testing_video,
            (len(rgb_feature_list)*len(tMatrices)*dims, 2048, 3), dims)
        print("accuracy for the current experiment is", accuracy)
        acc_list.append(accuracy)
        if accuracy > best_acc:
            best_acc = accuracy
            np.save('/home/boy2/UCF101/ucf101_dataset/frame_features/seed.npy', [seed_1, seed_2])
            np.save('/home/boy2/UCF101/ucf101_dataset/frame_features/tras_m_1.npy', tran_m_1)
            np.save('/home/boy2/UCF101/ucf101_dataset/frame_features/tras_m_1_norm.npy', tran_m_1_norm)
    print("Accuracy for each exp is:", acc_list)
    np.savetxt('/home/boy2/UCF101/ucf101_dataset/frame_features/result.txt', acc_list, delimiter=',')
